{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f18e3f9",
   "metadata": {},
   "source": [
    "# Bölüm Projesi: Sentiment Metin Sınıflandırma Modeli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a62613",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1FGhV9KhfPEgldczf2H-roiF75MvJEGca?hl=tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ccd401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "# from keras.preprocessing import text, sequence\n",
    "# from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74d7283c",
   "metadata": {},
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f31f98bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c8901d",
   "metadata": {},
   "source": [
    "veri etiketli (sentiment skorları ifade edilmiş) fakat ben bu etiketleri 2 sınıfa indirgeyeceğim yani yeniden metin üzerinden sentiment skorlarını oluşturacağım. yorumları pozitif yorum ve negatif yorum olacak şekilde 2 başlıkta değerlendirmeye çalışacağım. yani binary classification problemine dönüşecek. bu veri setinde negatif yorumlar 0 ve 1, pozitif yorumlar 3 ve 4 ile temsil edilmiştir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2236fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 ve 1 negatif olarak değiştirildi.\n",
    "# inplace = True işlemin kalıcı olmasını sağladı.\n",
    "data[\"Sentiment\"].replace(0, value = \"negatif\", inplace = True)\n",
    "data[\"Sentiment\"].replace(1, value = \"negatif\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9022a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Sentiment\"].replace(3, value = \"pozitif\", inplace = True)\n",
    "data[\"Sentiment\"].replace(4, value = \"pozitif\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e2d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481537fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 sınıfını kaldırıyorum\n",
    "data = data[(data.Sentiment == \"negatif\") | (data.Sentiment == \"pozitif\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a91dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"Sentiment\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af379d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f2b705",
   "metadata": {},
   "source": [
    "Veri Ön İşleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3440918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sadece lazım olan değişkenlerle yeni bir df oluşturdum.\n",
    "df = pd.DataFrame()\n",
    "df[\"text\"] = data[\"Phrase\"]\n",
    "df[\"label\"] = data[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd6129",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e94a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# küçük dönüşümü\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join(word.lower() for word in x.split()))\n",
    "#noktalama işaretleri\n",
    "df['text'] = df['text'].str.replace('[^\\w\\s]','')\n",
    "#sayılar\n",
    "df['text'] = df['text'].str.replace('\\d','')\n",
    "\n",
    "#stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join(word for word in x.split() if word not in sw))\n",
    "\n",
    "#seyreklerin silinmesi\n",
    "sil = pd.Series(' '.join(df['text']).split()).value_counts()[-1000:]\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join(word for word in x.split() if word not in sil))\n",
    "\n",
    "#lemmi\n",
    "from textblob import Word\n",
    "nltk.download('wordnet')\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join(Word(e).lemmatize() for e in x.split())) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8648b33",
   "metadata": {},
   "source": [
    "Değişken Mühendisliği\n",
    "\n",
    "Elimizdeki metinleri nümerik olarak temsil edebilmek adına bu metinlerden bazı nümerik bilgiler çıkarmamız gereklidir. bir metnin içerisinden özellik çıkaracağız (değişken oluşturacağız)\n",
    "\n",
    "Kullanılan Yöntemler:\n",
    "\n",
    "Count Vectors: her bir satır, bir yorumu ya da dökümanı temsil eder. Her bir sütun ise her bir dökümandaki kelimeleri ifade eder. kesişimlerinde yer alan ifadeler ise dökümanlarda geçme sıklığını ifade eder.\n",
    "\n",
    "TF-IDF Vectors (words, characters, n-grams):\n",
    "\n",
    "TF(t) = (Bir t teriminin bir dökümanda gözlenme frekansı) / (dökümandaki toplam terim sayısı)\n",
    "IDF(t) = log_e(Toplam döküman sayısı / içinde t terimi olan belge sayısı)\n",
    "Word Embeddings: kelime ve dökümanları yoğunluk vektörü gösterimi ile temsil etmenin bir yoludur. yani bir kelimenin vektör uzayındaki pozisyonu metinden öğreniliyor ve bu kelimenin konumu kendini çevreleyen kelimelere bağlı olduğundan dolayı burada kelime ve dökümanlar için bir yoğunluk vektörü oluşmuş olur. Embeddings, kullandığımız veri seti kullanılarak eğitilebilir ya da önceden eğitilmiş Word Embeddings'ler kullanılarak bu işlem gerçekleştirilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1952c37",
   "metadata": {},
   "source": [
    "Test-Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a3ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(df[\"text\"],\n",
    "                                                                    df[\"label\"],\n",
    "                                                                    random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ddc206",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564910cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bağımlı değişkene (y) bir dönüştürme işlemi yapacağım\n",
    "# 1 ve 0 olarak değiştireceğim\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "test_y = encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a4ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817fcf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba826f7",
   "metadata": {},
   "source": [
    "Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c17a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_count = vectorizer.transform(train_x)\n",
    "x_test_count = vectorizer.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7544e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c6232",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_count.toarray()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6fafb",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdf7fce",
   "metadata": {},
   "source": [
    "word level tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0989c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_word_vectorizer = TfidfVectorizer()\n",
    "tf_idf_word_vectorizer.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2703671",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tf_idf_word = tf_idf_word_vectorizer.transform(train_x)\n",
    "x_test_tf_idf_word = tf_idf_word_vectorizer.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b441baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_word_vectorizer.get_feature_names_out()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee2e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tf_idf_word.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456055b6",
   "metadata": {},
   "source": [
    "count'dan farkı ise göreceli etkilerdir yani döküman ve veri seti bazında görece etkileri göz önünde bulundurarak bu değerleri oluşturmuş olur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac40b2",
   "metadata": {},
   "source": [
    "n-gram level tf-idf\n",
    "\n",
    "kelimelerin birbirleriyle kombinasyonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c62b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_ngram_vectorizer = TfidfVectorizer(ngram_range = (2,3))\n",
    "tf_idf_ngram_vectorizer.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1470d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tf_idf_ngram = tf_idf_ngram_vectorizer.transform(train_x)\n",
    "x_test_tf_idf_ngram = tf_idf_ngram_vectorizer.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f075156f",
   "metadata": {},
   "source": [
    "characters level tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a8a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_chars_vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range = (2,3))\n",
    "tf_idf_chars_vectorizer.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tf_idf_chars = tf_idf_chars_vectorizer.transform(train_x)\n",
    "x_test_tf_idf_chars = tf_idf_chars_vectorizer.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec90516",
   "metadata": {},
   "source": [
    "count, word, ngram ve character ile feature'ları oluşturmuş olduk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30304e7",
   "metadata": {},
   "source": [
    "Makine Öğrenmesi ile Sentiment Sınıflandırması\n",
    "bir yorum geldiğinde bu yorumun pozitif mi yoksa negatif mi olduğunu tahmin edecek bir model kurmak\n",
    "\n",
    "her model için 4 farklı feature enginnering yöntemiyle deneme yapacağım"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc143ab",
   "metadata": {},
   "source": [
    "Lojistik Regresyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3098c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "loj = linear_model.LogisticRegression()\n",
    "loj_model = loj.fit(x_train_count, train_y)\n",
    "accuracy = model_selection.cross_val_score(loj_model, \n",
    "                                           x_test_count, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"Count Vectors Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e7995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loj = linear_model.LogisticRegression()\n",
    "loj_model = loj.fit(x_train_tf_idf_word,train_y)\n",
    "accuracy = model_selection.cross_val_score(loj_model, \n",
    "                                           x_test_tf_idf_word, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"Word-Level TF-IDF Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ba7a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "loj = linear_model.LogisticRegression()\n",
    "loj_model = loj.fit(x_train_tf_idf_ngram,train_y)\n",
    "accuracy = model_selection.cross_val_score(loj_model, \n",
    "                                           x_test_tf_idf_ngram, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"N-GRAM TF-IDF Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a28d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loj = linear_model.LogisticRegression()\n",
    "loj_model = loj.fit(x_train_tf_idf_chars,train_y)\n",
    "accuracy = model_selection.cross_val_score(loj_model, \n",
    "                                           x_test_tf_idf_chars, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"CHARLEVEL Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51be306",
   "metadata": {},
   "source": [
    "Count Vectors Doğruluk Oranı: 0.8368200836820083\n",
    "Word-Level TF-IDF Doğruluk Oranı: 0.8331589958158995\n",
    "N-GRAM TF-IDF Doğruluk Oranı: 0.748326359832636\n",
    "CHARLEVEL Doğruluk Oranı: 0.7811715481171548\n",
    "En Yüksek doğruluk oranı, Count Vectors yöntemiyle bulundu. Count Vectors ve Word-Level daha başarılıdır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0cbe38",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb71f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = naive_bayes.MultinomialNB()\n",
    "nb_model = nb.fit(x_train_count,train_y)\n",
    "accuracy = model_selection.cross_val_score(nb_model, \n",
    "                                           x_test_count, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"Count Vectors Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0386b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = naive_bayes.MultinomialNB()\n",
    "nb_model = nb.fit(x_train_tf_idf_word,train_y)\n",
    "accuracy = model_selection.cross_val_score(nb_model, \n",
    "                                           x_test_tf_idf_word, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"Word-Level TF-IDF Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e19a428",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = naive_bayes.MultinomialNB()\n",
    "nb_model = nb.fit(x_train_tf_idf_ngram,train_y)\n",
    "accuracy = model_selection.cross_val_score(nb_model, \n",
    "                                           x_test_tf_idf_ngram, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"N-GRAM TF-IDF Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caa51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = naive_bayes.MultinomialNB()\n",
    "nb_model = nb.fit(x_train_tf_idf_chars,train_y)\n",
    "accuracy = model_selection.cross_val_score(nb_model, \n",
    "                                           x_test_tf_idf_chars, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"CHARLEVEL Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677cb12c",
   "metadata": {},
   "source": [
    "Count Vectors Doğruluk Oranı: 0.8332112970711296\n",
    "Word-Level TF-IDF Doğruluk Oranı: 0.835041841004184\n",
    "N-GRAM TF-IDF Doğruluk Oranı: 0.7685146443514643\n",
    "CHARLEVEL Doğruluk Oranı: 0.7557008368200837\n",
    "Count Vectors ve Word-Level daha başarılıdır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f71107",
   "metadata": {},
   "source": [
    "Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c0568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ensemble.RandomForestClassifier()\n",
    "rf_model = rf.fit(x_train_count,train_y)\n",
    "accuracy = model_selection.cross_val_score(rf_model, \n",
    "                                           x_test_count, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"Count Vectors Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c30c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ensemble.RandomForestClassifier()\n",
    "rf_model = rf.fit(x_train_tf_idf_word,train_y)\n",
    "accuracy = model_selection.cross_val_score(rf_model, \n",
    "                                           x_test_tf_idf_word, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"Word-Level TF-IDF Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ensemble.RandomForestClassifier()\n",
    "rf_model = loj.fit(x_train_tf_idf_ngram,train_y)\n",
    "accuracy = model_selection.cross_val_score(rf_model, \n",
    "                                           x_test_tf_idf_ngram, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"N-GRAM TF-IDF Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f6b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ensemble.RandomForestClassifier()\n",
    "rf_model = loj.fit(x_train_tf_idf_chars,train_y)\n",
    "accuracy = model_selection.cross_val_score(rf_model, \n",
    "                                           x_test_tf_idf_chars, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"CHARLEVEL Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0daddfe",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b55aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgboost.XGBClassifier()\n",
    "xgb_model = xgb.fit(x_train_count,train_y)\n",
    "accuracy = model_selection.cross_val_score(xgb_model, \n",
    "                                           x_test_count, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"Count Vectors Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f706ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgboost.XGBClassifier()\n",
    "xgb_model = xgb.fit(x_train_tf_idf_word,train_y)\n",
    "accuracy = model_selection.cross_val_score(xgb_model, \n",
    "                                           x_test_tf_idf_word, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"Word-Level TF-IDF Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18beba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgboost.XGBClassifier()\n",
    "xgb_model = xgb.fit(x_train_tf_idf_ngram,train_y)\n",
    "accuracy = model_selection.cross_val_score(xgb_model, \n",
    "                                           x_test_tf_idf_ngram, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"N-GRAM TF-IDF Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f6daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgboost.XGBClassifier()\n",
    "xgb_model = xgb.fit(x_train_tf_idf_chars,train_y)\n",
    "accuracy = model_selection.cross_val_score(xgb_model, \n",
    "                                           x_test_tf_idf_chars, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"CHARLEVEL Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9401368",
   "metadata": {},
   "source": [
    "Tahmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c9ec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeni yorumların pozitif mi negatif mi olduğunu\n",
    "# lojistik regresyon ve count vectors yöntemi ile tahmin edilecek\n",
    "loj = linear_model.LogisticRegression()\n",
    "loj_model = loj.fit(x_train_count, train_y)\n",
    "accuracy = model_selection.cross_val_score(loj_model, \n",
    "                                           x_test_count, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"Count Vectors Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1e72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tahmin edeceğimiz yorumu dönüştürelim\n",
    "yeni_yorum = pd.Series(\"this film is very nice and good i like it\")\n",
    "yeni_yorum2 = pd.Series(\"no not good look at that shit very bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc83c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = CountVectorizer()\n",
    "v.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a465803",
   "metadata": {},
   "outputs": [],
   "source": [
    "yeni_yorum = v.transform(yeni_yorum)\n",
    "yeni_yorum2 = v.transform(yeni_yorum2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c382a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeni_yorum için tahmin\n",
    "loj_model.predict(yeni_yorum)\n",
    "# pozitif sınıf 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a929d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeni_yorum2 için tahmin\n",
    "loj_model.predict(yeni_yorum2)\n",
    "# negatif sınıf 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f4ae37",
   "metadata": {},
   "source": [
    "%50 kötü bir oran rastgele yapılmış olsaydı yine %50 dönderdi.\n",
    "en sık kötü yorumlar olduğu için binary'e çevirdiğimizde 0 kötü yorumlar için atandı."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
